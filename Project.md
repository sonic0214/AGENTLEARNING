# èƒŒæ™¯
Goolgeä¸¾åŠäº†ä¸€ä¸ªAI Agenté©¬æ‹‰æ¾ï¼ŒåŸºäºGoogle ADKæ„å»ºAgenté¡¹ç›®ã€‚

# ç›¸å…³èµ„æ–™
ADK æ–‡æ¡£ï¼ˆé“¾æ¥ https://google.github.io/adk-docs/ï¼‰
ADK-Pythonï¼ˆé“¾æ¥ https://github.com/google/adk-pythonï¼‰
ADK ç¤ºä¾‹ä»£ç†ï¼ˆé“¾æ¥ https://github.com/google/adk-samplesï¼‰
ç‰¹å·¥å…¥é—¨åŒ…ï¼ˆé“¾æ¥ https://github.com/GoogleCloudPlatform/agent-starter-packï¼‰
A2A-Docsï¼ˆé“¾æ¥ https://a2a-protocol.org/latest/ï¼‰
A2A-Pythonï¼ˆé“¾æ¥ https://github.com/a2aproject/a2a-pythonï¼‰

ç¤¾åŒº
Reddit ä»£ç†å¼€å‘å·¥å…·åŒ…ï¼ˆé“¾æ¥ https://www.reddit.com/r/agentdevelopmentkit/ï¼‰

æ–‡æ¡£
Vertex AI ä»£ç†å¼•æ“æ–‡æ¡£ï¼ˆé“¾æ¥ https://docs.cloud.google.com/agent-builder/agent-engine/overviewï¼‰


# æ¯”èµ›é¡¹ç›®ä»‹ç»
Description
This capstone project is an opportunity for you to apply what you've learned during the course week and submit it for evaluation.

The high level process for a capstone project consists of:

Selecting a track (and only one track) from one of the four options. This will scope the problem/use case for your project.
Example: Concierge Agents
Formulating a problem and solution pitch to which you will build your agent. This will hone the objective of your project.
Example: Writing blogs is too manual and time intensive. I will be building an Automated Blog Writer Agent to scale my blog production and blog quality.
Developing and publishing the code for your agent publicly.
Example: Publishing the agent code to GitHub or Kaggle Notebooks.
Being able to articulate the value of the agent by preparing a writeup.
Example: This agent reduced my blog writing process by 10 hours per week
For bonus points, publishing a video related to the project.
Submitting a writeup to this competition no later than December 1, 2025 11:59AM Pacific Time. Check out "How Do I Make A Submission section" below.

# é¡¹ç›®å†…å®¹è¦æ±‚
Features to Include in Your Agent Submission
In your submission, you must demonstrate what youâ€™ve learned in this course by applying at least three (3) of the key concepts listed below:

Multi-agent system, including any combination of:
Agent powered by an LLM
Parallel agents
Sequential agents
Loop agents
Tools, including:
MCP
custom tools
built-in tools, such as Google Search or Code Execution
OpenAPI tools
Long-running operations (pause/resume agents)
Sessions & Memory
Sessions & state management (e.g. InMemorySessionService)
Long term memory (e.g. Memory Bank)
Context engineering (e.g. context compaction)
Observability: Logging, Tracing, Metrics
Agent evaluation
A2A Protocol
Agent deployment


# è¯„åˆ†è§„åˆ™
Evaluation
Your submission will be evaluated on three categories earning up to a maximum of 100 points.

Category 1: The Pitch (max 30 points)
Category 2: The Implementation (max 70 points)
Bonus: (max 20 points)
Note: Bonus points are added to the total of Category 1 and 2 points, up to a maximum of total 100 points.

Example scoring evaluation: Category 1 score of 30 points + Category 2 score of 60 points + Bonus points of 20 = 100 points.

Criteria (points)	Description
Category 1: The Pitch (Problem, Solution, Value)
(30 points total)	This is where you'll be evaluated on the "why" and "what" of your project and how well you communicate your vision.
Core Concept & Value
(15 points)	Your project's central idea, its relevance to the track for the submission; focused on innovation and value. The use of agents should be clear, meaningful and central to your solution.
Writeup
(15 points)	How well your written submission articulates the problem you're solving, your solution, its architecture, and your project's journey.
Category 2: The Implementation
(Architecture, Code)
(70 points total)	This is where you'll be evaluated on the "how" of your project. This includes the quality of your code, technical design, and AI integration.
Technical Implementation
(50 points)	In your submission, you must demonstrate what youâ€™ve learned in this course by applying at least three (3) of the key concepts listed in the Features To Include In Your Agent Submission section.

For this criteria, we will assess the quality of your solution's architecture, and your code, and the meaningful use of agents in your solution.

Your code should contain comments pertinent to implementation, design and behaviors.

Participants are not required to deploy their agents to a live public endpoint for judging purposes; however, if you do deploy, please provide documentation to reproduce the deployment.

ğŸš¨REMINDER: DO NOT INCLUDE ANY API KEYS OR PASSWORDS IN YOUR CODE.
Documentation
(20 points)	Your submission (when submitting via GitHub) should contain a README.md file explaining the problem, solution, architecture, instructions for setup, and relevant diagrams or images where appropriate.

If you are solely submitting a Kaggle notebook, please provide documentation directly inline via Markdown Cells of the notebook.
Bonus points (Tooling, Model Use, Deployment, Video)
20 points total	You can earn optional bonus points.
Effective Use of Gemini
(5 points)	Use of Gemini to power your agent (or at least one sub-agent).
Agent Deployment
(5 points)	If you either have code or otherwise show evidence (e.g. in your code or write up) of having deployed your agent using Agent Engine or a similar Cloud-based runtime (e.g. Cloud Run).
YouTube Video Submission
(10 points)	Your video should include clarity, conciseness and quality of messaging. It should be under 3 min long. It should articulate:

Problem Statement: Describe the problem you're trying to solve, and why you think it's an important or interesting problem to solve.

Agents: Why agents? How can agents uniquely help solve that problem?

Architecture: Images and a description of the overall agent architecture.

Demo: demo of your solution, which can include images, an animation, or a video of the agent working.

The Build: How you created it, what tools or technologies you used.