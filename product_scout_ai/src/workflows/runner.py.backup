"""
Pipeline runner for executing analysis workflows.

This module provides the execution engine for running
analysis pipelines with ADK.
"""
from typing import Dict, Any, Optional, AsyncGenerator, Callable
from dataclasses import dataclass
from datetime import datetime
import asyncio
import json
import uuid
import logging

from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService, Session

from src.config.settings import Settings
from src.schemas.input_schemas import AnalysisRequest
from src.schemas.output_schemas import (
    TrendAnalysis,
    MarketAnalysis,
    CompetitionAnalysis,
    ProfitAnalysis,
    EvaluationResult,
    FinalReport,
)
from src.schemas.state_schemas import AnalysisState
from src.agents import extract_json_from_response
from src.utils.logger import get_logger, log_phase_start, log_phase_complete, log_agent_call

from .analysis_pipeline import AnalysisPipeline, PipelineResult


@dataclass
class RunnerConfig:
    """
    Configuration for the pipeline runner.

    Attributes:
        app_name: Application name for sessions
        max_retries: Maximum retry attempts for failed phases
        timeout_seconds: Timeout for each phase
        enable_streaming: Whether to enable streaming responses
    """
    app_name: str = "product_scout_ai"
    max_retries: int = 3
    timeout_seconds: int = 120
    enable_streaming: bool = True


class PipelineRunner:
    """
    Execution engine for analysis pipelines.

    Handles the actual execution of pipelines using ADK's
    Runner and Session management.
    """

    def __init__(
        self,
        settings: Optional[Settings] = None,
        config: Optional[RunnerConfig] = None
    ):
        """
        Initialize the runner.

        Args:
            settings: Application settings
            config: Runner configuration
        """
        self.settings = settings or Settings()
        self.config = config or RunnerConfig()
        self.logger = get_logger("pipeline_runner")

        # Initialize session service
        self._session_service = InMemorySessionService()

        # Pipeline instance
        self._pipeline: Optional[AnalysisPipeline] = None

        # Current session
        self._current_session: Optional[Session] = None

    async def create_session(self, user_id: Optional[str] = None) -> Session:
        """
        Create a new session for analysis.

        Args:
            user_id: Optional user identifier

        Returns:
            New Session instance
        """
        session_id = str(uuid.uuid4())
        user = user_id or "anonymous"

        session = await self._session_service.create_session(
            app_name=self.config.app_name,
            user_id=user,
            session_id=session_id
        )

        self._current_session = session
        return session

    def get_session(self, session_id: str) -> Optional[Session]:
        """
        Get an existing session.

        Args:
            session_id: Session ID

        Returns:
            Session if found, None otherwise
        """
        return self._session_service.get_session(
            app_name=self.config.app_name,
            user_id="*",  # Any user
            session_id=session_id
        )

    def initialize_pipeline(
        self,
        on_phase_complete: Optional[Callable[[str, Dict[str, Any]], None]] = None
    ) -> AnalysisPipeline:
        """
        Initialize the analysis pipeline.

        Args:
            on_phase_complete: Callback for phase completion

        Returns:
            Initialized pipeline
        """
        self._pipeline = AnalysisPipeline(
            settings=self.settings,
            on_phase_complete=on_phase_complete
        )
        return self._pipeline

    async def run_analysis(
        self,
        request: AnalysisRequest,
        session: Optional[Session] = None
    ) -> PipelineResult:
        """
        Run a complete analysis.

        This method orchestrates the full analysis pipeline:
        1. Initialize state
        2. Run parallel analysis phase
        3. Run evaluation phase
        4. Generate report

        Args:
            request: Analysis request
            session: Optional existing session

        Returns:
            PipelineResult with final state and report
        """
        start_time = datetime.now()
        phase_times: Dict[str, float] = {}

        # Create or use session
        if session is None:
            session = await self.create_session()

        # Initialize state
        state = AnalysisState(request=request)
        state.set_phase("initialized")

        # Initialize pipeline if needed
        if self._pipeline is None:
            self.initialize_pipeline()

        try:
            # Phase 1: Parallel Analysis
            phase_start = datetime.now()
            state.set_phase("analyzing_trends")
            log_phase_start(self.logger, "parallel_analysis", "Running parallel agents (trend, market, competition, profit)")

            # Create pipeline agents
            self.logger.info("ðŸ“¦ Creating pipeline agents...")
            pipeline_agents = self._pipeline.create_pipeline_agents(request)
            self.logger.info(f"âœ… Created {len(pipeline_agents)} pipeline agents")

            # Create ADK runner for parallel agent
            self.logger.info("ðŸƒ Initializing ADK Runner for parallel execution...")
            parallel_runner = Runner(
                agent=pipeline_agents["parallel_agent"],
                app_name=self.config.app_name,
                session_service=self._session_service
            )

            # Run parallel analysis using ADK
            self.logger.info("ðŸš€ Executing parallel analysis agents...")
            log_agent_call(self.logger, "ParallelAgent", f"Analyzing category: {request.category}")

            try:
                # Import enhanced ADK logging
                from src.utils.adk_logging import create_adk_logger

                # Create enhanced logger for ADK events
                adk_logger = create_adk_logger(self.logger, debug_mode=True)

                # Actually run the agents using ADK
                events = []
                event_count = 0

                # Use synchronous iteration to consume the generator
                # Create message object with role attribute for ADK
                class SimpleMessage:
                    def __init__(self, content):
                        self.content = content
                        self.role = "user"

                message = SimpleMessage(f"è¯·åˆ†æžäº§å“ç±»åˆ« '{request.category}' åœ¨å¸‚åœº '{request.target_market}' çš„æœºä¼š")

                self.logger.info("ðŸ“¡ Starting detailed ADK event logging...")

                for event in parallel_runner.run(
                    user_id="system",
                    session_id=session.id if hasattr(session, 'id') else str(uuid.uuid4()),
                    new_message=message
                ):
                    events.append(event)
                    event_count += 1

                    # Enhanced event logging with ADK logger
                    adk_logger.log_event(event, event_count)

                    # Keep the original debug log for compatibility
                    if self.logger.isEnabledFor(logging.DEBUG):
                        self.logger.debug(f"ðŸ“¨ Agent event {event_count}: {type(event).__name__} - {str(event)[:100] if len(str(event)) > 100 else str(event)}")

                # Log execution summary
                adk_logger.log_summary()

                # Collect final result from events (safely handle empty list)
                result = events[-1] if events and len(events) > 0 else None
                self.logger.info(f"âœ… Processed {event_count} agent events")

                if result:
                    self.logger.info(f"ðŸ“Š Final result type: {type(result).__name__}")
                    self.logger.info(f"ðŸ“Š Agent response length: {len(str(result))} chars")

                    # Try to extract individual agent outputs
                    agent_outputs = adk_logger.extract_agent_outputs()
                    for agent_name, output in agent_outputs.items():
                        self.logger.info(f"ðŸŽ¯ {agent_name.upper()} OUTPUT CAPTURED:")
                        self.logger.info(f"   Length: {len(output)} characters")
                        self.logger.info(f"   Preview: {output[:200]}..." if len(output) > 200 else output)

                else:
                    self.logger.warning("âš ï¸ No agent events were processed - agents may not have executed")

                    # TODO: Parse and store results in state
                    # state.trend_analysis = parse_trend_result(result)
                    # state.market_analysis = parse_market_result(result)
                    # state.competition_analysis = parse_competition_result(result)
                    # state.profit_analysis = parse_profit_result(result)

            except Exception as e:
                self.logger.error(f"âŒ Agent execution failed: {str(e)}", exc_info=True)
                raise

            elapsed = (datetime.now() - phase_start).total_seconds()
            phase_times["parallel_analysis"] = elapsed
            log_phase_complete(self.logger, "parallel_analysis", elapsed)

            # Phase 2: Evaluation (would run after parallel results)
            phase_start = datetime.now()
            state.set_phase("evaluating")
            log_phase_start(self.logger, "evaluation", "Evaluating opportunity score and generating recommendations")

            # TODO: Implement evaluation agent execution
            self.logger.info("âš ï¸  Evaluation phase not yet fully implemented")

            elapsed = (datetime.now() - phase_start).total_seconds()
            phase_times["evaluation"] = elapsed
            log_phase_complete(self.logger, "evaluation", elapsed)

            # Phase 3: Report Generation
            phase_start = datetime.now()
            state.set_phase("generating_report")
            log_phase_start(self.logger, "report_generation", "Generating final report")

            # TODO: Implement report generation
            self.logger.info("âš ï¸  Report generation phase not yet fully implemented")

            elapsed = (datetime.now() - phase_start).total_seconds()
            phase_times["report_generation"] = elapsed
            log_phase_complete(self.logger, "report_generation", elapsed)

            # Mark complete
            state.set_phase("completed")
            self.logger.info("ðŸŽ‰ Pipeline execution completed")

            execution_time = (datetime.now() - start_time).total_seconds()

            return PipelineResult(
                success=True,
                state=state,
                execution_time=execution_time,
                phase_times=phase_times
            )

        except Exception as e:
            state.set_error(str(e))
            execution_time = (datetime.now() - start_time).total_seconds()

            return PipelineResult(
                success=False,
                state=state,
                error=str(e),
                execution_time=execution_time,
                phase_times=phase_times
            )

    async def run_with_streaming(
        self,
        request: AnalysisRequest,
        session: Optional[Session] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Run analysis with streaming progress updates.

        Args:
            request: Analysis request
            session: Optional existing session

        Yields:
            Progress updates as dictionaries
        """
        start_time = datetime.now()

        # Create session
        if session is None:
            session = await self.create_session()

        # Initialize
        state = AnalysisState(request=request)

        yield {
            "type": "started",
            "phase": "initialized",
            "message": "Analysis started",
            "timestamp": datetime.now().isoformat()
        }

        # Initialize pipeline
        if self._pipeline is None:
            self.initialize_pipeline()

        # Phase updates
        phases = [
            ("analyzing_trends", "Analyzing market trends..."),
            ("analyzing_market", "Analyzing market size..."),
            ("analyzing_competition", "Analyzing competition..."),
            ("analyzing_profit", "Analyzing profitability..."),
            ("evaluating", "Evaluating opportunity..."),
            ("generating_report", "Generating report..."),
        ]

        for phase, message in phases:
            state.set_phase(phase)
            yield {
                "type": "progress",
                "phase": phase,
                "message": message,
                "timestamp": datetime.now().isoformat()
            }
            # Simulate phase execution
            await asyncio.sleep(0.1)

        state.set_phase("completed")

        yield {
            "type": "completed",
            "phase": "completed",
            "message": "Analysis complete",
            "execution_time": (datetime.now() - start_time).total_seconds(),
            "timestamp": datetime.now().isoformat()
        }

    def process_agent_output(
        self,
        agent_name: str,
        output: str,
        state: AnalysisState
    ) -> AnalysisState:
        """
        Process output from an agent and update state.

        Args:
            agent_name: Name of the agent
            output: Raw output from agent
            state: Current analysis state

        Returns:
            Updated analysis state
        """
        data = extract_json_from_response(output)

        if data is None:
            return state

        try:
            if agent_name == "trend_agent":
                state.trend_analysis = TrendAnalysis.from_dict(data)
            elif agent_name == "market_agent":
                state.market_analysis = MarketAnalysis.from_dict(data)
            elif agent_name == "competition_agent":
                state.competition_analysis = CompetitionAnalysis.from_dict(data)
            elif agent_name == "profit_agent":
                state.profit_analysis = ProfitAnalysis.from_dict(data)
            elif agent_name == "evaluator_agent":
                state.evaluation_result = EvaluationResult.from_dict(data)
        except (ValueError, KeyError) as e:
            # Log error but don't fail
            pass

        state.update_timestamp()
        return state


def create_runner(
    settings: Optional[Settings] = None,
    config: Optional[RunnerConfig] = None
) -> PipelineRunner:
    """
    Factory function to create a pipeline runner.

    Args:
        settings: Application settings
        config: Runner configuration

    Returns:
        Configured PipelineRunner
    """
    return PipelineRunner(settings, config)


async def quick_analyze(
    category: str,
    target_market: str = "US",
    business_model: str = "amazon_fba",
    budget_range: str = "medium"
) -> PipelineResult:
    """
    Quick analysis helper function.

    Args:
        category: Product category
        target_market: Target market
        business_model: Business model
        budget_range: Budget range

    Returns:
        Analysis result
    """
    request = AnalysisRequest(
        category=category,
        target_market=target_market,
        business_model=business_model,
        budget_range=budget_range
    )

    runner = create_runner()
    return await runner.run_analysis(request)
